## Voraussetzungen: Dieses Projekt nutzt Ollama als lokale KI-Runtime.
 Bitte Ollama von https://ollama.com installieren und anschließend das benötigte Modell mit `ollama pull llama3.1:8b` herunterladen 
 (Details: https://ollama.com/library/llama3.1:8b). 
 
 Stelle sicher, dass Ollama läuft (Standard: http://localhost:11434); Test mit `ollama run llama3.1:8b`.
